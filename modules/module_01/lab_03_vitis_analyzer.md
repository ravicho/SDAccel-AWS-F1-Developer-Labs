
# Application Performance Analysis


  - [Analyzing the Reports](#analyzing-the-reports)
    - [Profile Summary Report](#profile-summary-report)
    - [Vitis Link Summary and HLS reports](#vitis-link-summary-and-hls-reports)
    - [Application Timeline report](#application-timeline-report)
      - [Host Application Timeline](#host-application-timeline)
      - [Device Execution Timeline](#device-execution-timeline)
  - [Summary](#summary)

This lab discusses the different types of reports produced by the Vitis software platform during emulation and FPGA/hardware runs. These reports will be used to analyze performance metrics and identify potential for performance improvement.

>**NOTE**: This lab will utilize most of the emulation results created by different runs performed in the last lab. If you have not completed it, complete it first.

## Analyzing the Reports  

This section covers how to locate and read the various reports generated by the emulation runs. The goal of this section is to understand the analysis reports generated by Vitis before utilizing them in next sections.  

### Profile Summary Report

After hardware emulation run completes, a run summary is generated in the `build_hw_emu` folder. The summary can be opened with Vitis Analyzer.

1. Open the generated run summary report as follows:

  ```
  vitis_analyzer ./build_hw_emu/xclbin.run_summary
  ```

  After Vitis_analyzer opens run summary, from left hand side pan select "**Profile Summary**" to open it in the main window on right hand side.
  
 ![](images/module_01/lab_03_idct/hwEmuProfileSummary.PNG)
  
  This report provides application run data. Notice that the report has four tabs at the top: **Top Operations**, **Kernels & Compute Units**, **Data Transfers**, and **OpenCL APIs**.

2. Click through and inspect each of the tabs:

  * **Top Operations**: Top operations tab provides metrics in terms of completion/execution times for kernel and data transfers between host and the device global memories giving at glance view of any potential performance bottlenecks. This allows to identify throughput bottlenecks when transferring data or during kernel execution which ever is the slowest. Efficient transfer of data to the kernel/host allows for faster execution times.

  * **Kernels & Compute Units**: Shows the number of times the kernel was executed. Includes the total, minimum, average, and maximum run times. If the design has multiple compute units, it will show each compute unitâ€™s utilization. When accelerating an algorithm, the faster the kernel executes, the higher the throughput which can be achieved. It is best to optimize the kernel to be as fast as it can be with the data it requires. In this table some numbers give key insight into potential for performance improvement. CU unit utilization being small essentially hints that CU was not busy processing data most of the time but waiting for data to arrive from host.

  * **Data Transfers**: This tab has no bearing in software emulation as no actual data transfers are emulated across the host to the platform. In hardware emulation, this shows the throughput and bandwidth of the read/writes to the global memory that the host and kernel share. During hardware emulation these number give relatively accurate numbers, but generally hardware emulation is very slow and data set size used are smaller hence these number doesnt reflect very accurate picture. These number will be used for analysis in a realistic fashion in next lab while running on hardware where experiments with larger data sets are performed.

  * **OpenCL APIs**: Shows all the OpenCL API command executions, how many time each was executed, and how long they take to execute or how much time was spent during which this call was active.

3. Click on the **Kernels & Compute Units** tab of the **Profile Summary** report, locate and note down the Kernel Total Time (ms).


### Vitis Link Summary and HLS reports

The Vitis compiler also generates HLS Reports for each kernel. HLS Reports contain the results of compiling kernel into hardware. They contains many details (including clocking, resources or device utilization) about the performance and logic usage of the custom-generated hardware accelerator. These details provide many insights to guide the kernel optimization process. To build an xclbin file ( FPGA Binary File) with four different kernels, it will need four different reports for each kernel. For an the actual application, only one kernel is required, the other three are added for experimental purposes as pointed out in the previous lab. Next labs will use them for experiments. This lab also explores the use of Vitis Link Summary which contains all of these reports.

1.  Opening the link summary:

    ```bash
    cd $LAB_WORK_DIR/Vitis-AWS-F1-Developer-Labs/modules/module_01/idct/
    vitis_analyzer build_hw_emu/krnl_idct.hw_emu.awsxclbin.link_summary
    ```
    
    The figure below shows the main window opened by vitis_analyzer:
    
     ![](images/module_01/lab_03_idct/hwEmuLinkSummary.PNG)
     
     In the left panel, it first lists details of link summary, then compiles a summary for each kernel. First have a look at details from the link summary. In the left panel, click **System Diagram** to display the system diagram as shown below:
      
    ![](images/module_01/lab_03_idct/hwEmuLinkSummarySysDia.PNG)
    
    This diagram shows the number of compute units and how they connect to different memory banks and host PCIe connections.
    
    The Vitis Analyzer also provides important guidance about the kernel compilation or HLS process which can be seen by selecting **System Guidance** as shown in figure below:
    
    ![](images/module_01/lab_03_idct/hwEmuSysGuidance.PNG)
    
    System guidance provides important information about hardware compile and link process for every kernel. In this case guidance about **burst inference** on different kernel memory ports is provided. The messages say that multiple read and write bursts of variable length are inferred on kernel ports. _Burst memory transfer over AXI interfaces generally have better throughput than non-burst transfers_.

2. Next open synthesis reports and compare them for kernel latencies and resource usage.
   
    - To do this first select "krnl_idct" to bring the report as shown in figure below. Note down latency(min/max) and resource utilization. From resource utilization table, the resource usage for the kernels in absolute numbers and percentage of total resources on FPGA can be seen. The main resources on the FPGA are Digital Signal Processing modules (DSPs), block RAM memory modules (BRAMs), Flip Flops (FFs), URAMs and Look up tables (LUTs).
    
    ![](images/module_01/lab_03_idct/hwEmuHlsSynReportFast.PNG)
   
   - Next, open the HLS synthesis report for "krnl_idct_med" and note down latency and resource utilization. The report for this kernel is as shown below. Note down the latency (min/max) and resource utilization
   
    ![](images/module_01/lab_03_idct/hwEmuHlsSynReportMed.PNG)
    
   - Finally, open the report for the third kernel namely "krnl_idct_slow" and note down latency(min/max) and resources.
         
From the latency numbers for all the kernels, it should be clear that the "krnl_idct" has the minimum latency and "krnl_idct_slow" has the maximum latency. These kernels are explicitly designed to have these latencies to perform some experiments.

Next, it can be seen how these kernels have been genertaed with minor differences in terms of **HLS Pragmas** used for loop pipelining and dataflow optimization. To do this:
    
1. Open the file source file `krnl_idct.cpp` and go to label "PIPELINE_PRAGMA" near line 297:
 
    ```bash
    vim $LAB_WORK_DIR/Vitis-AWS-F1-Developer-Labs/modules/module_01/idct/src/krnl_idct.cpp
    ```
   
   An HLS pragma is placed here for loop pipelining. Note the II=2 constraint, which means back to back loop iteration should start after every two cycles (for example, next loop iteration should start processing after two cycles of current iteration and they should continue execution in an overlapping fashion).
       
2. Open the file for the second kernel, `krnl_idct_slow.cpp`. Go to label "PIPELINE_PRAGMA" near line 297:

    ```bash
    vim $LAB_WORK_DIR/Vitis-AWS-F1-Developer-Labs/modules/module_01/idct/src/krnl_idct_slow.cpp
    ```
    
    It can be noticed here that the II constrains is 8. This constraint allows Vitis HLS tool to share resources if possible and hence decrease resource utilization. Similarly one can have a look at third kernel namely **krnl_idct_med** which has II=4 constraint and compare resources. Generally increasing II can reduce resources but it may not be a linear relationship depending on the availability of resources in the design itself with the potential for getting shared.
       
### Application Timeline report

In addition to the profile summary file, the emulation run also generates a timeline trace file as part of run summary. This gives more details about:
- Full application behavior
- Application interactions with FPGA
- Execution times on hardware side (FPGA Card).

The report can be analyzed for host side application issues and other things like:

 - Looking at specific data transfer rates
 - Kernel execution times for different enqueued operations.
 
 Essentially a timeline describes application lifetime with annotations for data transfer sizes, transfer times and bandwidth utilization. The timeline also gives event dependencies between different enqueued tasks such as memory transfers and kernel execution. These dependencies can be seen by simple mouse click on any task enqueue on host side timeline trace.  

Open the generated run summary report:

```
vitis_analyzer build_hw_emu/xclbin.run_summary 
```

After the Vitis_analyzer opens run summary, from left hand side pan select "**Application Timeline**" to open it in the main window on right hand side.

![](images/module_01/lab_03_idct/applicationTimelineHwEmu.PNG)


The Application Timeline shown above collects and displays host and device events on a common timeline to help understand and visualize the overall health and performance of the system. These events include OpenCL API calls from the host code, about when they happen and how long each of them takes. For example the rows marked by blue and yellow arrows trace read and write data movements from host side. The events inside blue and yellow circles show actual data transfers happening on this timeline. Application Timeline has two distinct sections for the host and the device.

#### Host Application Timeline

Host side uses multiple sets of OpenCL buffers. Each set contains two input and one output buffer sufficient for single kernel call. Multiple set of buffers help to enqueue multiple kernel calls at the same time. Host uses a pool of buffers based on a circular pointer. At the start host enqueues multiple kernel calls and corresponding memory transfers using this pool of buffers. Once the complete pool is exhausted, the host application checks for a set of buffers which is free after being used by any kernel call enqueued previously. If a set of free buffers is available for kernel input and output a new task is enqueued with new input data. The dependencies between data transfers and kernel execution are created in a fashion described below:

  * Data transfers from host to device global memory don't depend on anything and hence they can pretty much complete anytime before kernel execution which can be seen by zooming into the timeline as shown in the figure below and highlighted by yellow ellipse.
 
  * Kernel execution/enqueues depend on host to device data transfers and also on any kernel enqueue that was done before, so they complete after related transfers and previous kernel enqueues. In the timeline below yellow ellipse shows start of next kernel enqueue happening after data transfer and it also happens when previous kernel enqueue operation has finished marked in red ellipse.
  
  * Transfers from host to device depend on kernel execution so they always complete after kernel enqueue calls complete. It is shown by an operation in blue ellipse as it happens after previous kernel enqueue operation finishes marked by red ellipse.
  
    ![](images/module_01/lab_03_idct/kernEnqAfterRead.PNG) 
    
  All the **stated dependencies** can also be checked by clicking on any enqueue call it will **display arrow connections** which shows dependencies as specified by application programmer on host side using **OpenCL APIs** while enqueuing different operations.

#### Device Execution Timeline

Device side timeline trace gives details of activity happening on the FPGA device or acceleration card. Here actual hardware activity happening for different CUs can be found. For IDCT, there is only one instance of IDCT kernel so a single CU is used. All its interfaces to device global memory are traced out. In the case of IDCT it uses three separate interfaces one for co-efficients, second for input data and third for output data ( even though two of these interfaces for co-efficients and input data use same DDR memory bank). Zoom into one of the read/write transactions happening on device master AXI interfaces as shown in the figure below and see how fast these data transfers are happening and in which sort of bursts, the timeline will show different bursts and also burst lengths. Different colored ellipses are used to highlight interfaces and data movements happening on these interfaces:

* Green ellipse highlights read and write interfaces with same port names as used in kernel C/C++ description
* Yellow ellipse highlights movement of co-efficients
* Red ellipse highlights movement of input data from device memory to kernel and blocks within the transaction show different number of bursts that happened.
* Blue ellipse highlights data movement from kernel to device memory in different bursts, by hovering mouse on these burst it displays a tooltip with burst statistics ( kernel, compute, start, stop, size, data rate etc.).

 One can also note that since IDCT coefficients and input data use same DDR memory bank and happen in non-overlapping fashion whereas output write operation has some overlap with device read operation because it uses a separate DDR memory bank. The next labs, will show how maximizing this kind of overlap considerably improves application performance.


![](images/module_01/lab_03_idct/memTxHwEmuDevice.PNG) 


## Summary  

In this lab, you learned the following:

- Reading different type of reports produced by the Vitis software platform
- About the Vitis Link Summary
- Reading these reports for important metrics
- Analyzing timeline traces for host and device efficiency and potential improvements

In the next labs, you will utilize these analysis capabilities to drive and measure code optimizations both for host and kernel performance improvements.

---------------------------------------

<p align="center"><b>
Start the next lab: <a href="lab_04_idct_optimization.md">IDCT Kernel Hardware Optimizations and Performance Analysis</a>
<p align="center"><sup>Copyright&copy; 2020 Xilinx</sup></p>
</b></p>  
 
